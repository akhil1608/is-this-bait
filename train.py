import torchimport pandas as pdfrom datasets import Datasetfrom transformers import (    AutoTokenizer,    AutoModelForSequenceClassification,    TrainingArguments,    Trainer)# Set pathsMODEL_NAME = "./models/base/distilbert-base-uncased"MODEL_DIR = "./models/trained/bait-v1"DATA_PATH = "./data/sports_bait_dataset_400.csv"# Load dataset (expects two columns: 'text' and 'label')df = pd.read_csv(DATA_PATH)dataset = Dataset.from_pandas(df)# Load tokenizertokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)# Tokenize datasetdef tokenize_function(examples):    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=128)dataset = dataset.map(tokenize_function, batched=True)dataset = dataset.train_test_split(test_size=0.1)# Load modelmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)  # Binary classification# Training setuptraining_args = TrainingArguments(    output_dir="./results",    evaluation_strategy="epoch",    save_strategy="epoch",    per_device_train_batch_size=8,    per_device_eval_batch_size=8,    num_train_epochs=3,    logging_dir="./logs",    logging_steps=10,    save_total_limit=2)trainer = Trainer(    model=model,    args=training_args,    train_dataset=dataset["train"],    eval_dataset=dataset["test"],    tokenizer=tokenizer)# Train modeltrainer.train()# Save trained modeltrainer.save_model(MODEL_DIR)tokenizer.save_pretrained(MODEL_DIR)print(f"Model saved to {MODEL_DIR}")